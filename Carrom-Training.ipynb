{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "792319da-2f52-4c47-a374-414cf626e05d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3164dc79-7153-47b6-aef0-cfed5510d040",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2198758 entries, 0 to 2198757\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Dtype \n",
      "---  ------      ----- \n",
      " 0   Unnamed: 0  int64 \n",
      " 1   Coin 0      object\n",
      " 2   Coin 1      object\n",
      " 3   Coin 2      object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 67.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('coin_positions.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81e196c9-3516-4b49-abce-f5620f0e3287",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coin 0</th>\n",
       "      <th>Coin 1</th>\n",
       "      <th>Coin 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.25--63.23712636181692</td>\n",
       "      <td>89.25--29.75</td>\n",
       "      <td>89.25--98.31284390112754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.5--63.47425272363384</td>\n",
       "      <td>88.5--30.5</td>\n",
       "      <td>88.5--97.62568780225507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.75--63.71137908545076</td>\n",
       "      <td>87.75--31.25</td>\n",
       "      <td>87.75--96.93853170338261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.0--63.94850544726768</td>\n",
       "      <td>87.0--32.0</td>\n",
       "      <td>87.0--96.25137560451014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.25--64.1856318090846</td>\n",
       "      <td>86.25--32.75</td>\n",
       "      <td>86.25--95.56421950563768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Coin 0        Coin 1                    Coin 2\n",
       "0  80.25--63.23712636181692  89.25--29.75  89.25--98.31284390112754\n",
       "1   79.5--63.47425272363384    88.5--30.5   88.5--97.62568780225507\n",
       "2  78.75--63.71137908545076  87.75--31.25  87.75--96.93853170338261\n",
       "3   78.0--63.94850544726768    87.0--32.0   87.0--96.25137560451014\n",
       "4   77.25--64.1856318090846  86.25--32.75  86.25--95.56421950563768"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop(columns=[df.columns[0]])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20d5f44b-059e-4b55-a85a-6f3233ddcff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for coin in df.columns:\n",
    "    df[[f\"{coin}_x\", f\"{coin}_y\"]] = df[coin].str.split(\"--\", expand=True).astype(float)\n",
    "    # df[f\"{coin}_x\"] = df[f\"{coin}_x\"].round(8)\n",
    "    # df[f\"{coin}_y\"] = df[f\"{coin}_y\"].round(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80f6ea12-24e6-4aaa-9856-1d11d760e672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coin 0</th>\n",
       "      <th>Coin 1</th>\n",
       "      <th>Coin 2</th>\n",
       "      <th>Coin 0_x</th>\n",
       "      <th>Coin 0_y</th>\n",
       "      <th>Coin 1_x</th>\n",
       "      <th>Coin 1_y</th>\n",
       "      <th>Coin 2_x</th>\n",
       "      <th>Coin 2_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.25--63.23712636181692</td>\n",
       "      <td>89.25--29.75</td>\n",
       "      <td>89.25--98.31284390112754</td>\n",
       "      <td>80.25</td>\n",
       "      <td>63.237126</td>\n",
       "      <td>89.25</td>\n",
       "      <td>29.75</td>\n",
       "      <td>89.25</td>\n",
       "      <td>98.312844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.5--63.47425272363384</td>\n",
       "      <td>88.5--30.5</td>\n",
       "      <td>88.5--97.62568780225507</td>\n",
       "      <td>79.50</td>\n",
       "      <td>63.474253</td>\n",
       "      <td>88.50</td>\n",
       "      <td>30.50</td>\n",
       "      <td>88.50</td>\n",
       "      <td>97.625688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.75--63.71137908545076</td>\n",
       "      <td>87.75--31.25</td>\n",
       "      <td>87.75--96.93853170338261</td>\n",
       "      <td>78.75</td>\n",
       "      <td>63.711379</td>\n",
       "      <td>87.75</td>\n",
       "      <td>31.25</td>\n",
       "      <td>87.75</td>\n",
       "      <td>96.938532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.0--63.94850544726768</td>\n",
       "      <td>87.0--32.0</td>\n",
       "      <td>87.0--96.25137560451014</td>\n",
       "      <td>78.00</td>\n",
       "      <td>63.948505</td>\n",
       "      <td>87.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>87.00</td>\n",
       "      <td>96.251376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.25--64.1856318090846</td>\n",
       "      <td>86.25--32.75</td>\n",
       "      <td>86.25--95.56421950563768</td>\n",
       "      <td>77.25</td>\n",
       "      <td>64.185632</td>\n",
       "      <td>86.25</td>\n",
       "      <td>32.75</td>\n",
       "      <td>86.25</td>\n",
       "      <td>95.564220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Coin 0        Coin 1                    Coin 2  Coin 0_x  \\\n",
       "0  80.25--63.23712636181692  89.25--29.75  89.25--98.31284390112754     80.25   \n",
       "1   79.5--63.47425272363384    88.5--30.5   88.5--97.62568780225507     79.50   \n",
       "2  78.75--63.71137908545076  87.75--31.25  87.75--96.93853170338261     78.75   \n",
       "3   78.0--63.94850544726768    87.0--32.0   87.0--96.25137560451014     78.00   \n",
       "4   77.25--64.1856318090846  86.25--32.75  86.25--95.56421950563768     77.25   \n",
       "\n",
       "    Coin 0_y  Coin 1_x  Coin 1_y  Coin 2_x   Coin 2_y  \n",
       "0  63.237126     89.25     29.75     89.25  98.312844  \n",
       "1  63.474253     88.50     30.50     88.50  97.625688  \n",
       "2  63.711379     87.75     31.25     87.75  96.938532  \n",
       "3  63.948505     87.00     32.00     87.00  96.251376  \n",
       "4  64.185632     86.25     32.75     86.25  95.564220  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b229423-ab4c-4b0e-9b71-689571a6a154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coin 0</th>\n",
       "      <th>Coin 1</th>\n",
       "      <th>Coin 2</th>\n",
       "      <th>Coin 0_x</th>\n",
       "      <th>Coin 0_y</th>\n",
       "      <th>Coin 1_x</th>\n",
       "      <th>Coin 1_y</th>\n",
       "      <th>Coin 2_x</th>\n",
       "      <th>Coin 2_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.25--63.23712636181692</td>\n",
       "      <td>89.25--29.75</td>\n",
       "      <td>89.25--98.31284390112754</td>\n",
       "      <td>80.25</td>\n",
       "      <td>63.237126</td>\n",
       "      <td>89.25</td>\n",
       "      <td>29.75</td>\n",
       "      <td>89.25</td>\n",
       "      <td>98.312844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.5--63.47425272363384</td>\n",
       "      <td>88.5--30.5</td>\n",
       "      <td>88.5--97.62568780225507</td>\n",
       "      <td>79.50</td>\n",
       "      <td>63.474253</td>\n",
       "      <td>88.50</td>\n",
       "      <td>30.50</td>\n",
       "      <td>88.50</td>\n",
       "      <td>97.625688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.75--63.71137908545076</td>\n",
       "      <td>87.75--31.25</td>\n",
       "      <td>87.75--96.93853170338261</td>\n",
       "      <td>78.75</td>\n",
       "      <td>63.711379</td>\n",
       "      <td>87.75</td>\n",
       "      <td>31.25</td>\n",
       "      <td>87.75</td>\n",
       "      <td>96.938532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.0--63.94850544726768</td>\n",
       "      <td>87.0--32.0</td>\n",
       "      <td>87.0--96.25137560451014</td>\n",
       "      <td>78.00</td>\n",
       "      <td>63.948505</td>\n",
       "      <td>87.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>87.00</td>\n",
       "      <td>96.251376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.25--64.1856318090846</td>\n",
       "      <td>86.25--32.75</td>\n",
       "      <td>86.25--95.56421950563768</td>\n",
       "      <td>77.25</td>\n",
       "      <td>64.185632</td>\n",
       "      <td>86.25</td>\n",
       "      <td>32.75</td>\n",
       "      <td>86.25</td>\n",
       "      <td>95.564220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Coin 0        Coin 1                    Coin 2  Coin 0_x  \\\n",
       "0  80.25--63.23712636181692  89.25--29.75  89.25--98.31284390112754     80.25   \n",
       "1   79.5--63.47425272363384    88.5--30.5   88.5--97.62568780225507     79.50   \n",
       "2  78.75--63.71137908545076  87.75--31.25  87.75--96.93853170338261     78.75   \n",
       "3   78.0--63.94850544726768    87.0--32.0   87.0--96.25137560451014     78.00   \n",
       "4   77.25--64.1856318090846  86.25--32.75  86.25--95.56421950563768     77.25   \n",
       "\n",
       "    Coin 0_y  Coin 1_x  Coin 1_y  Coin 2_x   Coin 2_y  \n",
       "0  63.237126     89.25     29.75     89.25  98.312844  \n",
       "1  63.474253     88.50     30.50     88.50  97.625688  \n",
       "2  63.711379     87.75     31.25     87.75  96.938532  \n",
       "3  63.948505     87.00     32.00     87.00  96.251376  \n",
       "4  64.185632     86.25     32.75     86.25  95.564220  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6ec7850-7661-42dc-9212-9fce3fa4fb47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=df.drop(columns=df.columns[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5d65742-aacc-45f7-b645-ee62e8d181da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coin 0_x</th>\n",
       "      <th>Coin 0_y</th>\n",
       "      <th>Coin 1_x</th>\n",
       "      <th>Coin 1_y</th>\n",
       "      <th>Coin 2_x</th>\n",
       "      <th>Coin 2_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.25</td>\n",
       "      <td>63.237126</td>\n",
       "      <td>89.25</td>\n",
       "      <td>29.75</td>\n",
       "      <td>89.25</td>\n",
       "      <td>98.312844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.50</td>\n",
       "      <td>63.474253</td>\n",
       "      <td>88.50</td>\n",
       "      <td>30.50</td>\n",
       "      <td>88.50</td>\n",
       "      <td>97.625688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.75</td>\n",
       "      <td>63.711379</td>\n",
       "      <td>87.75</td>\n",
       "      <td>31.25</td>\n",
       "      <td>87.75</td>\n",
       "      <td>96.938532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.00</td>\n",
       "      <td>63.948505</td>\n",
       "      <td>87.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>87.00</td>\n",
       "      <td>96.251376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.25</td>\n",
       "      <td>64.185632</td>\n",
       "      <td>86.25</td>\n",
       "      <td>32.75</td>\n",
       "      <td>86.25</td>\n",
       "      <td>95.564220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Coin 0_x   Coin 0_y  Coin 1_x  Coin 1_y  Coin 2_x   Coin 2_y\n",
       "0     80.25  63.237126     89.25     29.75     89.25  98.312844\n",
       "1     79.50  63.474253     88.50     30.50     88.50  97.625688\n",
       "2     78.75  63.711379     87.75     31.25     87.75  96.938532\n",
       "3     78.00  63.948505     87.00     32.00     87.00  96.251376\n",
       "4     77.25  64.185632     86.25     32.75     86.25  95.564220"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2fdd85a-a57b-44aa-954f-c086b65a260c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "sequence_length = 5\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequence = data[i:i + seq_length]\n",
    "        sequences.append(sequence)\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "sequences = create_sequences(data, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "452c39e6-1100-44ba-ba57-9baa7306a4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TensorDataset(sequences)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3758de2f-3151-42aa-bf3e-89e6f6038562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class AutoencoderLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, sequence_length):\n",
    "        super(AutoencoderLSTM, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, latent_dim)\n",
    "        self.lstm = nn.LSTM(latent_dim, latent_dim, batch_first=True)\n",
    "        self.decoder = Decoder(latent_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, input_dim = x.size()\n",
    "        x = x.view(batch_size * seq_len, input_dim)  # Flatten for encoding\n",
    "        encoded = self.encoder(x)  # Encode each frame\n",
    "        encoded = encoded.view(batch_size, seq_len, -1)  # Reshape for LSTM\n",
    "        lstm_out, _ = self.lstm(encoded)  # Process with LSTM\n",
    "        decoded = self.decoder(lstm_out[:, -1, :])  # Decode only last step\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e678f74b-6c8f-46bb-b7a2-f056948d9221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "input_dim = 6  # Number of x, y positions (per coin)\n",
    "latent_dim = 64\n",
    "sequence_length = 5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = AutoencoderLSTM(input_dim=input_dim, latent_dim=latent_dim, sequence_length=sequence_length).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "369cff31-c14b-4126-a713-556d5198dab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 746.2786789552159\n",
      "Epoch [5/30], Loss: 59.79487263829874\n",
      "Epoch [6/30], Loss: 45.13632757863207\n",
      "Epoch [7/30], Loss: 33.91725440772218\n",
      "Epoch [8/30], Loss: 26.58611898006927\n",
      "Epoch [9/30], Loss: 22.041707214790062\n",
      "Epoch [10/30], Loss: 20.50082183377365\n",
      "Epoch [11/30], Loss: 18.899644665290914\n",
      "Epoch [12/30], Loss: 17.564322362881942\n",
      "Epoch [13/30], Loss: 16.403425108811625\n",
      "Epoch [14/30], Loss: 16.32355385845831\n",
      "Epoch [15/30], Loss: 15.2603948959652\n",
      "Epoch [16/30], Loss: 15.185990816957514\n",
      "Epoch [17/30], Loss: 13.706486382490514\n",
      "Epoch [18/30], Loss: 13.292922259273917\n",
      "Epoch [19/30], Loss: 12.925332653884947\n",
      "Epoch [20/30], Loss: 13.41690026911189\n",
      "Epoch [21/30], Loss: 12.272624859662965\n",
      "Epoch [22/30], Loss: 13.015739100938658\n",
      "Epoch [23/30], Loss: 12.031150908230202\n",
      "Epoch [24/30], Loss: 11.620155580550168\n",
      "Epoch [25/30], Loss: 11.66984432343985\n",
      "Epoch [26/30], Loss: 11.563688932168313\n",
      "Epoch [27/30], Loss: 11.463320853957146\n",
      "Epoch [28/30], Loss: 11.390864067202173\n",
      "Epoch [29/30], Loss: 12.409747164278395\n",
      "Epoch [30/30], Loss: 11.044624263500182\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for sequences in train_loader:\n",
    "        sequences = sequences[0].to(device)  # Extract data from DataLoader\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sequences)\n",
    "        target = sequences[:, -1, :]  # Actual next frame\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe83dc-0f01-4381-a701-681f46ab2b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction example\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sequences in train_loader:\n",
    "        sequences = sequences[0].to(device)\n",
    "        output = model(sequences)\n",
    "        print(\"Predicted next frame:\", output.cpu().numpy())\n",
    "        break  # Remove this if you want to loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0032a6c7-5706-4cb6-9329-79e39d6e2118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4e11240-2b18-4e44-b4ab-b8a8f0f4f502",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (5x5 and 8x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_predictions):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# Predict the next frame\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m         predicted_frame \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (1, num_coins * 2)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# Format the predicted frame as \"x--y\" for each coin\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         formatted_frame \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m, in \u001b[0;36mAutoencoderLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m batch_size, seq_len, input_dim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m seq_len, input_dim)  \u001b[38;5;66;03m# Flatten for encoding\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Encode each frame\u001b[39;00m\n\u001b[1;32m     39\u001b[0m encoded \u001b[38;5;241m=\u001b[39m encoded\u001b[38;5;241m.\u001b[39mview(batch_size, seq_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape for LSTM\u001b[39;00m\n\u001b[1;32m     40\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(encoded)  \u001b[38;5;66;03m# Process with LSTM\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (5x5 and 8x64)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your model and DataLoader (train_loader) are already set up\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Parameters\n",
    "num_predictions = 500000  # Number of frames to predict\n",
    "\n",
    "# Get the initial sequence from the train_loader (first batch)\n",
    "initial_sequence = None\n",
    "for batch in train_loader:\n",
    "    initial_sequence = batch[0]  # Assuming batch[0] contains the frames\n",
    "    break  # Use only the first batch\n",
    "\n",
    "initial_sequence = initial_sequence.to(device)  # Shape: (batch_size, sequence_length, num_coins * 2)\n",
    "input_sequence = initial_sequence[0].unsqueeze(0)  # Shape: (1, sequence_length, num_coins * 2)\n",
    "\n",
    "# Determine the number of coins based on input shape\n",
    "sequence_length, num_features = input_sequence.shape[1], input_sequence.shape[2]\n",
    "num_coins = num_features // 2\n",
    "\n",
    "# Initialize list to store predictions\n",
    "predicted_frames = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_predictions):\n",
    "        # Predict the next frame\n",
    "        predicted_frame = model(input_sequence)  # Shape: (1, num_coins * 2)\n",
    "        \n",
    "        # Format the predicted frame as \"x--y\" for each coin\n",
    "        formatted_frame = []\n",
    "        for i in range(num_coins):\n",
    "            x = predicted_frame[0, i * 2].item()\n",
    "            y = predicted_frame[0, i * 2 + 1].item()\n",
    "            formatted_frame.append(f\"{x:.8f}--{y:.8f}\")\n",
    "        \n",
    "        # Append the formatted frame to the list for saving later\n",
    "        predicted_frames.append(formatted_frame)\n",
    "        \n",
    "        # Update the input sequence by removing the oldest frame and appending the new prediction\n",
    "        input_sequence = torch.cat([input_sequence[:, 1:], predicted_frame.unsqueeze(1)], dim=1)\n",
    "\n",
    "# Convert predicted frames to DataFrame\n",
    "column_names = [f\"Coin {i}\" for i in range(num_coins)]\n",
    "predicted_frames_df = pd.DataFrame(predicted_frames, columns=column_names)\n",
    "\n",
    "# Save to CSV\n",
    "predicted_frames_df.to_csv(\"coin_positions2.csv\", index=True)\n",
    "print(\"Predicted frames saved to predicted_carrom_frames.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64cac65f-af3a-4dd3-88cc-0f35a85d3656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd301e3-9286-4858-b068-008572b121ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
